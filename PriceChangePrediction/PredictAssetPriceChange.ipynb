{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredictAssetPriceChange.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBvWfI-CFGbm",
        "colab_type": "text"
      },
      "source": [
        "# An Attempt to Predict Future Asset Price Change\n",
        "(This notebook is created by [liyinnbw](https://github.com/liyinnbw/ML/tree/master/PriceChangePrediction) under the MIT license)\n",
        "\n",
        "Problem Definition:\n",
        "\n",
        "* Given a time series of past N days of an asset's trading data (including price, volume, etc.) predict the average price change in next M days."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PQiDZ9CGXc3",
        "colab_type": "text"
      },
      "source": [
        "## Load Data\n",
        "The sample data is 20 year daily trading data of the PowerShares QQQ Trust"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqF0UfdKkqoM",
        "colab_type": "code",
        "outputId": "c29beb3e-80a6-497b-cdc6-673c6fc0549f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_o = pd.read_csv('https://raw.githubusercontent.com/liyinnbw/ML/master/PriceChangePrediction/daily_QQQ.csv')\n",
        "df = df_o.sort_values(['timestamp'])\n",
        "df = df.reset_index(drop=True)\n",
        "df"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000-02-03</td>\n",
              "      <td>188.60</td>\n",
              "      <td>193.300</td>\n",
              "      <td>184.6000</td>\n",
              "      <td>193.00</td>\n",
              "      <td>15289900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000-02-04</td>\n",
              "      <td>194.00</td>\n",
              "      <td>196.500</td>\n",
              "      <td>192.4000</td>\n",
              "      <td>194.30</td>\n",
              "      <td>13320000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2000-02-07</td>\n",
              "      <td>194.50</td>\n",
              "      <td>197.300</td>\n",
              "      <td>192.5000</td>\n",
              "      <td>197.00</td>\n",
              "      <td>8880500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000-02-08</td>\n",
              "      <td>198.80</td>\n",
              "      <td>205.800</td>\n",
              "      <td>198.8000</td>\n",
              "      <td>205.30</td>\n",
              "      <td>10999700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000-02-09</td>\n",
              "      <td>204.10</td>\n",
              "      <td>204.400</td>\n",
              "      <td>197.5000</td>\n",
              "      <td>198.00</td>\n",
              "      <td>13629100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5027</th>\n",
              "      <td>2020-01-29</td>\n",
              "      <td>222.66</td>\n",
              "      <td>222.930</td>\n",
              "      <td>220.8300</td>\n",
              "      <td>221.81</td>\n",
              "      <td>29063128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5028</th>\n",
              "      <td>2020-01-30</td>\n",
              "      <td>220.37</td>\n",
              "      <td>222.700</td>\n",
              "      <td>219.6891</td>\n",
              "      <td>222.60</td>\n",
              "      <td>39568093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5029</th>\n",
              "      <td>2020-01-31</td>\n",
              "      <td>223.50</td>\n",
              "      <td>223.560</td>\n",
              "      <td>218.2850</td>\n",
              "      <td>219.07</td>\n",
              "      <td>52015832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5030</th>\n",
              "      <td>2020-02-03</td>\n",
              "      <td>220.14</td>\n",
              "      <td>222.885</td>\n",
              "      <td>219.9900</td>\n",
              "      <td>222.38</td>\n",
              "      <td>25468307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5031</th>\n",
              "      <td>2020-02-04</td>\n",
              "      <td>225.39</td>\n",
              "      <td>227.860</td>\n",
              "      <td>224.6523</td>\n",
              "      <td>227.47</td>\n",
              "      <td>30104089</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5032 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       timestamp    open     high       low   close    volume\n",
              "0     2000-02-03  188.60  193.300  184.6000  193.00  15289900\n",
              "1     2000-02-04  194.00  196.500  192.4000  194.30  13320000\n",
              "2     2000-02-07  194.50  197.300  192.5000  197.00   8880500\n",
              "3     2000-02-08  198.80  205.800  198.8000  205.30  10999700\n",
              "4     2000-02-09  204.10  204.400  197.5000  198.00  13629100\n",
              "...          ...     ...      ...       ...     ...       ...\n",
              "5027  2020-01-29  222.66  222.930  220.8300  221.81  29063128\n",
              "5028  2020-01-30  220.37  222.700  219.6891  222.60  39568093\n",
              "5029  2020-01-31  223.50  223.560  218.2850  219.07  52015832\n",
              "5030  2020-02-03  220.14  222.885  219.9900  222.38  25468307\n",
              "5031  2020-02-04  225.39  227.860  224.6523  227.47  30104089\n",
              "\n",
              "[5032 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqtsu-QfG8v0",
        "colab_type": "text"
      },
      "source": [
        "## Compute Daily Fractional Changes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQuir2xzJkvG",
        "colab_type": "code",
        "outputId": "a1e0f6b6-7100-422b-cd92-d1340d2b36b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "df_processed = pd.DataFrame()\n",
        "df_processed[['daily_price_change', 'daily_volume_change']] = df[['close', 'volume']].pct_change()\n",
        "\n",
        "pred_days = 5\n",
        "df_processed['average_price_change_in_next_{}_days'.format(pred_days)] = df['close'].rolling(window=pred_days).mean().shift(-pred_days)/df['close']-1\n",
        "df_processed = df_processed.dropna()\n",
        "df_processed"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>daily_price_change</th>\n",
              "      <th>daily_volume_change</th>\n",
              "      <th>average_price_change_in_next_5_days</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.006736</td>\n",
              "      <td>-0.128837</td>\n",
              "      <td>0.034071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.013896</td>\n",
              "      <td>-0.333296</td>\n",
              "      <td>0.022437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.042132</td>\n",
              "      <td>0.238635</td>\n",
              "      <td>-0.023186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.035558</td>\n",
              "      <td>0.239043</td>\n",
              "      <td>0.014848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.032828</td>\n",
              "      <td>-0.033627</td>\n",
              "      <td>-0.017604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5022</th>\n",
              "      <td>0.002642</td>\n",
              "      <td>-0.003709</td>\n",
              "      <td>-0.009559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5023</th>\n",
              "      <td>0.003216</td>\n",
              "      <td>0.277758</td>\n",
              "      <td>-0.014506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5024</th>\n",
              "      <td>-0.008415</td>\n",
              "      <td>0.412905</td>\n",
              "      <td>-0.009403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5025</th>\n",
              "      <td>-0.020656</td>\n",
              "      <td>0.128963</td>\n",
              "      <td>0.015415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5026</th>\n",
              "      <td>0.015360</td>\n",
              "      <td>-0.367509</td>\n",
              "      <td>0.005491</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5026 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      daily_price_change  ...  average_price_change_in_next_5_days\n",
              "1               0.006736  ...                             0.034071\n",
              "2               0.013896  ...                             0.022437\n",
              "3               0.042132  ...                            -0.023186\n",
              "4              -0.035558  ...                             0.014848\n",
              "5               0.032828  ...                            -0.017604\n",
              "...                  ...  ...                                  ...\n",
              "5022            0.002642  ...                            -0.009559\n",
              "5023            0.003216  ...                            -0.014506\n",
              "5024           -0.008415  ...                            -0.009403\n",
              "5025           -0.020656  ...                             0.015415\n",
              "5026            0.015360  ...                             0.005491\n",
              "\n",
              "[5026 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXMsITsvi0oL",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Features & Labels\n",
        "* Input feature (x): N days history of features up till today (dimension: ?, N, 2)\n",
        "* Output label (y): The next day close price change from today (dimension: ?, 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLwVhIBgznme",
        "colab_type": "code",
        "outputId": "c076b43b-6a6c-46d3-cc1d-38f7ce718a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "history_days = 20\n",
        "\n",
        "features = df_processed.iloc[:, :-1].values \n",
        "x = np.array([features[i:i+history_days] for i in range(0, len(features)-history_days+1)])\n",
        "y = df_processed.iloc[:,-1].values[history_days-1:].reshape(-1,1)\n",
        "\n",
        "# print(x)\n",
        "# print(y)\n",
        "print(x.shape, y.shape)\n",
        "print('fraction of samples with positive price change in future = {}'.format(len(np.where(y>0)[0])/len(y)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5007, 20, 2) (5007, 1)\n",
            "fraction of samples with positive price movement = 0.5562212901937288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLR5ic5jN1Zn",
        "colab_type": "text"
      },
      "source": [
        "## Check Label Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxt79jOpN_LG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "62ee8b45-8aa8-45fa-c039-aceccbe9b2ef"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_ = plt.hist(y, bins='auto')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/h/OrY9em7GEzAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sarHaI9DqM52",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Training and Testing data\n",
        "Because stock has bear and bull markets, we should sample randomly from the full data to get test samples. Otherwise, the test result may not be representative enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZmmW4zbqy7i",
        "colab_type": "code",
        "outputId": "2bca5b03-2629-438a-dcbc-95f2a99dfed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "totalsamples = x.shape[0]\n",
        "testsamples = 300\n",
        "idxs = np.arange(totalsamples)\n",
        "np.random.shuffle(idxs)\n",
        "\n",
        "x_train = x[idxs][:-testsamples]\n",
        "y_train = y[idxs][:-testsamples]\n",
        "x_test = x[idxs][-testsamples:]\n",
        "y_test = y[idxs][-testsamples:]\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4707, 20, 2) (4707, 1)\n",
            "(300, 20, 2) (300, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDxTHZWM7yuN",
        "colab_type": "text"
      },
      "source": [
        "## NetGenerator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF-y4Qad70hf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "f9460823-ef8f-47d7-c2c0-e80221f0ddc2"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import optimizers\n",
        "\n",
        "class NetGenerator():\n",
        "  def __init__(self, input_shape, out_size):\n",
        "    self.input_shape = input_shape\n",
        "    self.out_size = out_size\n",
        "\n",
        "  def next(self):\n",
        "\n",
        "    modelnames =[\n",
        "      'lstm-20day-history',\n",
        "    ]\n",
        "     \n",
        "    for idx, name in enumerate(modelnames):\n",
        "      \n",
        "      model = None\n",
        "      inputs = keras.Input(shape=self.input_shape)\n",
        "    #   x = keras.layers.Conv1D(filters=20, kernel_size=3, strides=1,padding='same')(inputs)\n",
        "    #   x = keras.layers.BatchNormalization()(x)\n",
        "    #   x = keras.layers.Activation('relu')(x)\n",
        "\n",
        "      x = keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(inputs)\n",
        "      x = keras.layers.BatchNormalization()(x)\n",
        "      x = keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=False)(x)\n",
        "      x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "      x = keras.layers.Dense(self.out_size)(x)\n",
        "      x = keras.layers.Activation('tanh')(x)\n",
        "      model = keras.Model(inputs=inputs, outputs=x)\n",
        "      model.compile(\n",
        "          loss='mean_squared_error',\n",
        "          optimizer=optimizers.Adam(lr=0.001)\n",
        "      )\n",
        "\n",
        "      yield (name, model)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khlN1qpgDgjV",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "Uncomment the following for training stats visualization when using colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYs2Ivh7Dh8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install tensorboardcolab\n",
        "# from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "\n",
        "# tbc=TensorBoardColab(port=6007, graph_path='.', startup_waiting_time=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4XoRe89wXB8",
        "colab_type": "code",
        "outputId": "f52d8956-e8b9-4e4f-a991-6128cae83d61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import keras\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from keras import backend as K \n",
        "import tensorflow as tf\n",
        "\n",
        "print (keras.__version__)\n",
        "print (tf.__version__)\n",
        "\n",
        "net_gen = NetGenerator(\n",
        "    input_shape=x_train[0].shape, \n",
        "    out_size=y_train[0].shape[0], \n",
        ")\n",
        "\n",
        "# very important or error\n",
        "K.clear_session()\n",
        "net_generator = net_gen.next()\n",
        "modelconfig=next(net_generator,None)\n",
        "  \n",
        "while modelconfig!=None:\n",
        "\n",
        "    modelname,model = modelconfig\n",
        "\n",
        "    print(modelname)\n",
        "    model.summary()\n",
        "\n",
        "    cb_tensorboard = TensorBoard(\n",
        "        log_dir='./log/'+modelname,\n",
        "        histogram_freq=100,  \n",
        "        write_graph=True, \n",
        "        write_images=False,\n",
        "        write_grads=False\n",
        "    )\n",
        "\n",
        "    cb_checkpoint = ModelCheckpoint(\n",
        "        './log/'+modelname+'_weights.h5', \n",
        "        monitor='val_loss', \n",
        "        verbose=0, \n",
        "        save_best_only=True, \n",
        "        save_weights_only=True, \n",
        "        mode='auto', \n",
        "        period=1\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        x=x_train, \n",
        "        y=y_train, \n",
        "        batch_size=256, \n",
        "        epochs=200, \n",
        "        verbose=1, \n",
        "        callbacks=[cb_tensorboard, cb_checkpoint], \n",
        "        validation_split=0.1, \n",
        "        shuffle=True, \n",
        "        initial_epoch=0, \n",
        "        steps_per_epoch=None, \n",
        "        validation_steps=None\n",
        "    )\n",
        "    \n",
        "    # very important or error\n",
        "    K.clear_session()\n",
        "    modelconfig=next(net_generator,None)\n",
        "\n",
        "print('All done')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.5\n",
            "1.15.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "lstm-20day-history\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 20, 2)             0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 20, 64)            17152     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 20, 64)            256       \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 32)                12416     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 29,985\n",
            "Trainable params: 29,793\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 4236 samples, validate on 471 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1120: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/200\n",
            "4236/4236 [==============================] - 3s 726us/step - loss: 0.2507 - val_loss: 0.0619\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "Epoch 2/200\n",
            "4236/4236 [==============================] - 1s 330us/step - loss: 0.0779 - val_loss: 0.0277\n",
            "Epoch 3/200\n",
            "4236/4236 [==============================] - 1s 326us/step - loss: 0.0375 - val_loss: 0.0074\n",
            "Epoch 4/200\n",
            "4236/4236 [==============================] - 1s 318us/step - loss: 0.0246 - val_loss: 0.0048\n",
            "Epoch 5/200\n",
            "4236/4236 [==============================] - 1s 324us/step - loss: 0.0195 - val_loss: 0.0031\n",
            "Epoch 6/200\n",
            "4236/4236 [==============================] - 1s 319us/step - loss: 0.0144 - val_loss: 0.0029\n",
            "Epoch 7/200\n",
            "4236/4236 [==============================] - 1s 321us/step - loss: 0.0107 - val_loss: 0.0029\n",
            "Epoch 8/200\n",
            "4236/4236 [==============================] - 1s 336us/step - loss: 0.0093 - val_loss: 0.0037\n",
            "Epoch 9/200\n",
            "4236/4236 [==============================] - 1s 327us/step - loss: 0.0079 - val_loss: 0.0034\n",
            "Epoch 10/200\n",
            "4236/4236 [==============================] - 1s 318us/step - loss: 0.0066 - val_loss: 0.0019\n",
            "Epoch 11/200\n",
            "4236/4236 [==============================] - 1s 333us/step - loss: 0.0056 - val_loss: 0.0016\n",
            "Epoch 12/200\n",
            "4236/4236 [==============================] - 1s 320us/step - loss: 0.0052 - val_loss: 0.0018\n",
            "Epoch 13/200\n",
            "4236/4236 [==============================] - 1s 325us/step - loss: 0.0046 - val_loss: 0.0018\n",
            "Epoch 14/200\n",
            "4236/4236 [==============================] - 1s 318us/step - loss: 0.0044 - val_loss: 0.0021\n",
            "Epoch 15/200\n",
            "4236/4236 [==============================] - 1s 316us/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 16/200\n",
            "4236/4236 [==============================] - 1s 323us/step - loss: 0.0036 - val_loss: 0.0015\n",
            "Epoch 17/200\n",
            "4236/4236 [==============================] - 1s 323us/step - loss: 0.0035 - val_loss: 0.0014\n",
            "Epoch 18/200\n",
            "4236/4236 [==============================] - 1s 321us/step - loss: 0.0032 - val_loss: 0.0016\n",
            "Epoch 19/200\n",
            "4236/4236 [==============================] - 1s 333us/step - loss: 0.0031 - val_loss: 0.0019\n",
            "Epoch 20/200\n",
            "4236/4236 [==============================] - 1s 330us/step - loss: 0.0030 - val_loss: 0.0016\n",
            "Epoch 21/200\n",
            "4236/4236 [==============================] - 1s 320us/step - loss: 0.0025 - val_loss: 0.0016\n",
            "Epoch 22/200\n",
            "4236/4236 [==============================] - 1s 324us/step - loss: 0.0025 - val_loss: 0.0015\n",
            "Epoch 23/200\n",
            "4236/4236 [==============================] - 1s 320us/step - loss: 0.0025 - val_loss: 0.0015\n",
            "Epoch 24/200\n",
            "4236/4236 [==============================] - 1s 325us/step - loss: 0.0023 - val_loss: 0.0017\n",
            "Epoch 25/200\n",
            "4236/4236 [==============================] - 1s 320us/step - loss: 0.0023 - val_loss: 0.0014\n",
            "Epoch 26/200\n",
            "4236/4236 [==============================] - 1s 323us/step - loss: 0.0023 - val_loss: 0.0014\n",
            "Epoch 27/200\n",
            "4236/4236 [==============================] - 1s 319us/step - loss: 0.0020 - val_loss: 0.0014\n",
            "Epoch 28/200\n",
            "4236/4236 [==============================] - 1s 320us/step - loss: 0.0021 - val_loss: 0.0015\n",
            "Epoch 29/200\n",
            "4236/4236 [==============================] - 1s 317us/step - loss: 0.0019 - val_loss: 0.0016\n",
            "Epoch 30/200\n",
            "4236/4236 [==============================] - 1s 322us/step - loss: 0.0017 - val_loss: 0.0013\n",
            "Epoch 31/200\n",
            "4236/4236 [==============================] - 1s 327us/step - loss: 0.0022 - val_loss: 0.0016\n",
            "Epoch 32/200\n",
            "4236/4236 [==============================] - 1s 322us/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 33/200\n",
            "4236/4236 [==============================] - 1s 318us/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 34/200\n",
            "4236/4236 [==============================] - 1s 327us/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 35/200\n",
            "4236/4236 [==============================] - 1s 321us/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 36/200\n",
            "4236/4236 [==============================] - 1s 323us/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 37/200\n",
            "4236/4236 [==============================] - 1s 322us/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 38/200\n",
            "4236/4236 [==============================] - 1s 318us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 39/200\n",
            "4236/4236 [==============================] - 1s 322us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 40/200\n",
            "4236/4236 [==============================] - 1s 321us/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 41/200\n",
            "4236/4236 [==============================] - 1s 340us/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 42/200\n",
            "4236/4236 [==============================] - 1s 319us/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 43/200\n",
            "4236/4236 [==============================] - 1s 327us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 44/200\n",
            "4236/4236 [==============================] - 1s 330us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 45/200\n",
            "4236/4236 [==============================] - 1s 325us/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 46/200\n",
            "4236/4236 [==============================] - 1s 330us/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 47/200\n",
            "4236/4236 [==============================] - 1s 339us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 48/200\n",
            "4236/4236 [==============================] - 1s 338us/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 49/200\n",
            "4236/4236 [==============================] - 1s 334us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 50/200\n",
            "4236/4236 [==============================] - 1s 318us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 51/200\n",
            "4236/4236 [==============================] - 1s 323us/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 52/200\n",
            "4236/4236 [==============================] - 1s 341us/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 53/200\n",
            "4236/4236 [==============================] - 1s 323us/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 54/200\n",
            "4236/4236 [==============================] - 1s 336us/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 55/200\n",
            "4236/4236 [==============================] - 1s 321us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 56/200\n",
            "4236/4236 [==============================] - 1s 323us/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 57/200\n",
            "4236/4236 [==============================] - 1s 316us/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 58/200\n",
            "4236/4236 [==============================] - 1s 320us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 59/200\n",
            "4236/4236 [==============================] - 1s 319us/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 60/200\n",
            "4236/4236 [==============================] - 1s 326us/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 61/200\n",
            "4236/4236 [==============================] - 1s 320us/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 62/200\n",
            "4236/4236 [==============================] - 1s 323us/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 63/200\n",
            "4236/4236 [==============================] - 1s 341us/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 64/200\n",
            "4236/4236 [==============================] - 1s 322us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 65/200\n",
            "4236/4236 [==============================] - 1s 322us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 66/200\n",
            "4236/4236 [==============================] - 1s 322us/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 67/200\n",
            "4236/4236 [==============================] - 1s 318us/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 68/200\n",
            "4236/4236 [==============================] - 1s 316us/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 69/200\n",
            "4236/4236 [==============================] - 1s 322us/step - loss: 9.5394e-04 - val_loss: 0.0013\n",
            "Epoch 70/200\n",
            "4236/4236 [==============================] - 1s 317us/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 71/200\n",
            "4236/4236 [==============================] - 1s 317us/step - loss: 9.4330e-04 - val_loss: 0.0012\n",
            "Epoch 72/200\n",
            "4236/4236 [==============================] - 1s 320us/step - loss: 9.9440e-04 - val_loss: 0.0013\n",
            "Epoch 73/200\n",
            "4236/4236 [==============================] - 1s 321us/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 74/200\n",
            "4236/4236 [==============================] - 1s 316us/step - loss: 9.4820e-04 - val_loss: 0.0013\n",
            "Epoch 75/200\n",
            "4236/4236 [==============================] - 1s 321us/step - loss: 9.5555e-04 - val_loss: 0.0013\n",
            "Epoch 76/200\n",
            "4236/4236 [==============================] - 1s 322us/step - loss: 9.1009e-04 - val_loss: 0.0012\n",
            "Epoch 77/200\n",
            "4236/4236 [==============================] - 1s 313us/step - loss: 9.6678e-04 - val_loss: 0.0012\n",
            "Epoch 78/200\n",
            "4236/4236 [==============================] - 1s 321us/step - loss: 9.4959e-04 - val_loss: 0.0012\n",
            "Epoch 79/200\n",
            "4236/4236 [==============================] - 1s 311us/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 80/200\n",
            "4236/4236 [==============================] - 1s 322us/step - loss: 9.9453e-04 - val_loss: 0.0014\n",
            "Epoch 81/200\n",
            "4236/4236 [==============================] - 1s 325us/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 82/200\n",
            "4236/4236 [==============================] - 1s 317us/step - loss: 0.0010 - val_loss: 0.0015\n",
            "Epoch 83/200\n",
            "4236/4236 [==============================] - 1s 323us/step - loss: 8.9765e-04 - val_loss: 0.0013\n",
            "Epoch 84/200\n",
            "4236/4236 [==============================] - 1s 331us/step - loss: 9.1038e-04 - val_loss: 0.0012\n",
            "Epoch 85/200\n",
            "4236/4236 [==============================] - 1s 324us/step - loss: 9.2509e-04 - val_loss: 0.0013\n",
            "Epoch 86/200\n",
            "4236/4236 [==============================] - 1s 315us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 87/200\n",
            "4236/4236 [==============================] - 1s 313us/step - loss: 8.9997e-04 - val_loss: 0.0015\n",
            "Epoch 88/200\n",
            "4236/4236 [==============================] - 1s 315us/step - loss: 9.7872e-04 - val_loss: 0.0017\n",
            "Epoch 89/200\n",
            "4236/4236 [==============================] - 1s 313us/step - loss: 9.6078e-04 - val_loss: 0.0014\n",
            "Epoch 90/200\n",
            "4236/4236 [==============================] - 1s 310us/step - loss: 9.4108e-04 - val_loss: 0.0012\n",
            "Epoch 91/200\n",
            "4236/4236 [==============================] - 1s 311us/step - loss: 8.7147e-04 - val_loss: 0.0012\n",
            "Epoch 92/200\n",
            "4236/4236 [==============================] - 1s 320us/step - loss: 9.2728e-04 - val_loss: 0.0013\n",
            "Epoch 93/200\n",
            "4236/4236 [==============================] - 1s 312us/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 94/200\n",
            "4236/4236 [==============================] - 1s 310us/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 95/200\n",
            "4236/4236 [==============================] - 1s 310us/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 96/200\n",
            "4236/4236 [==============================] - 1s 353us/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 97/200\n",
            "4236/4236 [==============================] - 1s 320us/step - loss: 9.4414e-04 - val_loss: 0.0013\n",
            "Epoch 98/200\n",
            "4236/4236 [==============================] - 1s 326us/step - loss: 8.7753e-04 - val_loss: 0.0012\n",
            "Epoch 99/200\n",
            "4236/4236 [==============================] - 1s 326us/step - loss: 8.3869e-04 - val_loss: 0.0012\n",
            "Epoch 100/200\n",
            "4236/4236 [==============================] - 1s 306us/step - loss: 8.7100e-04 - val_loss: 0.0013\n",
            "Epoch 101/200\n",
            "4236/4236 [==============================] - 1s 314us/step - loss: 8.3663e-04 - val_loss: 0.0014\n",
            "Epoch 102/200\n",
            "4236/4236 [==============================] - 1s 302us/step - loss: 8.7267e-04 - val_loss: 0.0013\n",
            "Epoch 103/200\n",
            "4236/4236 [==============================] - 1s 313us/step - loss: 8.0657e-04 - val_loss: 0.0012\n",
            "Epoch 104/200\n",
            "4236/4236 [==============================] - 1s 326us/step - loss: 8.9036e-04 - val_loss: 0.0012\n",
            "Epoch 105/200\n",
            "4236/4236 [==============================] - 1s 318us/step - loss: 8.6313e-04 - val_loss: 0.0012\n",
            "Epoch 106/200\n",
            "4236/4236 [==============================] - 1s 309us/step - loss: 9.0271e-04 - val_loss: 0.0012\n",
            "Epoch 107/200\n",
            "4236/4236 [==============================] - 1s 336us/step - loss: 9.0982e-04 - val_loss: 0.0014\n",
            "Epoch 108/200\n",
            "4236/4236 [==============================] - 1s 308us/step - loss: 8.8757e-04 - val_loss: 0.0012\n",
            "Epoch 109/200\n",
            "4236/4236 [==============================] - 1s 316us/step - loss: 9.5648e-04 - val_loss: 0.0013\n",
            "Epoch 110/200\n",
            "4236/4236 [==============================] - 1s 309us/step - loss: 8.4269e-04 - val_loss: 0.0013\n",
            "Epoch 111/200\n",
            "4236/4236 [==============================] - 1s 313us/step - loss: 8.2930e-04 - val_loss: 0.0014\n",
            "Epoch 112/200\n",
            "4236/4236 [==============================] - 1s 315us/step - loss: 9.4201e-04 - val_loss: 0.0012\n",
            "Epoch 113/200\n",
            "4236/4236 [==============================] - 1s 318us/step - loss: 8.9008e-04 - val_loss: 0.0014\n",
            "Epoch 114/200\n",
            "4236/4236 [==============================] - 1s 320us/step - loss: 8.7406e-04 - val_loss: 0.0013\n",
            "Epoch 115/200\n",
            "4236/4236 [==============================] - 1s 314us/step - loss: 8.1070e-04 - val_loss: 0.0012\n",
            "Epoch 116/200\n",
            "4236/4236 [==============================] - 1s 309us/step - loss: 8.2656e-04 - val_loss: 0.0012\n",
            "Epoch 117/200\n",
            "4236/4236 [==============================] - 1s 317us/step - loss: 9.3276e-04 - val_loss: 0.0013\n",
            "Epoch 118/200\n",
            "4236/4236 [==============================] - 1s 314us/step - loss: 8.0306e-04 - val_loss: 0.0012\n",
            "Epoch 119/200\n",
            "4236/4236 [==============================] - 1s 306us/step - loss: 8.2257e-04 - val_loss: 0.0013\n",
            "Epoch 120/200\n",
            "4236/4236 [==============================] - 1s 319us/step - loss: 8.9422e-04 - val_loss: 0.0014\n",
            "Epoch 121/200\n",
            "4236/4236 [==============================] - 1s 309us/step - loss: 8.3492e-04 - val_loss: 0.0015\n",
            "Epoch 122/200\n",
            "4236/4236 [==============================] - 1s 317us/step - loss: 8.2993e-04 - val_loss: 0.0012\n",
            "Epoch 123/200\n",
            "4236/4236 [==============================] - 1s 313us/step - loss: 8.6562e-04 - val_loss: 0.0015\n",
            "Epoch 124/200\n",
            "4236/4236 [==============================] - 1s 309us/step - loss: 8.3023e-04 - val_loss: 0.0012\n",
            "Epoch 125/200\n",
            "4236/4236 [==============================] - 1s 310us/step - loss: 8.1484e-04 - val_loss: 0.0012\n",
            "Epoch 126/200\n",
            "4236/4236 [==============================] - 1s 309us/step - loss: 8.5684e-04 - val_loss: 0.0012\n",
            "Epoch 127/200\n",
            "4236/4236 [==============================] - 1s 315us/step - loss: 8.0419e-04 - val_loss: 0.0013\n",
            "Epoch 128/200\n",
            "4236/4236 [==============================] - 1s 310us/step - loss: 8.4372e-04 - val_loss: 0.0012\n",
            "Epoch 129/200\n",
            "4236/4236 [==============================] - 1s 332us/step - loss: 8.5529e-04 - val_loss: 0.0013\n",
            "Epoch 130/200\n",
            "4236/4236 [==============================] - 1s 305us/step - loss: 8.3531e-04 - val_loss: 0.0012\n",
            "Epoch 131/200\n",
            "4236/4236 [==============================] - 1s 311us/step - loss: 9.0450e-04 - val_loss: 0.0014\n",
            "Epoch 132/200\n",
            "4236/4236 [==============================] - 1s 310us/step - loss: 8.4513e-04 - val_loss: 0.0014\n",
            "Epoch 133/200\n",
            "4236/4236 [==============================] - 1s 305us/step - loss: 8.2417e-04 - val_loss: 0.0013\n",
            "Epoch 134/200\n",
            "4236/4236 [==============================] - 1s 310us/step - loss: 8.1564e-04 - val_loss: 0.0012\n",
            "Epoch 135/200\n",
            "4236/4236 [==============================] - 1s 309us/step - loss: 8.4690e-04 - val_loss: 0.0016\n",
            "Epoch 136/200\n",
            "4236/4236 [==============================] - 1s 307us/step - loss: 8.1577e-04 - val_loss: 0.0012\n",
            "Epoch 137/200\n",
            "4236/4236 [==============================] - 1s 305us/step - loss: 8.5534e-04 - val_loss: 0.0013\n",
            "Epoch 138/200\n",
            "4236/4236 [==============================] - 1s 312us/step - loss: 8.1551e-04 - val_loss: 0.0013\n",
            "Epoch 139/200\n",
            "4236/4236 [==============================] - 1s 305us/step - loss: 7.6011e-04 - val_loss: 0.0013\n",
            "Epoch 140/200\n",
            "4236/4236 [==============================] - 1s 317us/step - loss: 9.1857e-04 - val_loss: 0.0012\n",
            "Epoch 141/200\n",
            "4236/4236 [==============================] - 1s 326us/step - loss: 7.6387e-04 - val_loss: 0.0013\n",
            "Epoch 142/200\n",
            "4236/4236 [==============================] - 1s 310us/step - loss: 7.6847e-04 - val_loss: 0.0012\n",
            "Epoch 143/200\n",
            "4236/4236 [==============================] - 1s 317us/step - loss: 7.4954e-04 - val_loss: 0.0012\n",
            "Epoch 144/200\n",
            "4236/4236 [==============================] - 1s 305us/step - loss: 8.2764e-04 - val_loss: 0.0014\n",
            "Epoch 145/200\n",
            "4236/4236 [==============================] - 1s 310us/step - loss: 8.3506e-04 - val_loss: 0.0014\n",
            "Epoch 146/200\n",
            "4236/4236 [==============================] - 1s 312us/step - loss: 8.2935e-04 - val_loss: 0.0018\n",
            "Epoch 147/200\n",
            "4236/4236 [==============================] - 1s 305us/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 148/200\n",
            "4236/4236 [==============================] - 1s 308us/step - loss: 8.3367e-04 - val_loss: 0.0013\n",
            "Epoch 149/200\n",
            "4236/4236 [==============================] - 1s 314us/step - loss: 8.1030e-04 - val_loss: 0.0013\n",
            "Epoch 150/200\n",
            "4236/4236 [==============================] - 1s 306us/step - loss: 7.6459e-04 - val_loss: 0.0013\n",
            "Epoch 151/200\n",
            "4236/4236 [==============================] - 1s 312us/step - loss: 7.9082e-04 - val_loss: 0.0012\n",
            "Epoch 152/200\n",
            "4236/4236 [==============================] - 1s 333us/step - loss: 7.5418e-04 - val_loss: 0.0012\n",
            "Epoch 153/200\n",
            "4236/4236 [==============================] - 1s 313us/step - loss: 7.4839e-04 - val_loss: 0.0012\n",
            "Epoch 154/200\n",
            "4236/4236 [==============================] - 1s 309us/step - loss: 7.8222e-04 - val_loss: 0.0012\n",
            "Epoch 155/200\n",
            "4236/4236 [==============================] - 1s 312us/step - loss: 7.4714e-04 - val_loss: 0.0012\n",
            "Epoch 156/200\n",
            "4236/4236 [==============================] - 1s 312us/step - loss: 8.8446e-04 - val_loss: 0.0013\n",
            "Epoch 157/200\n",
            "4236/4236 [==============================] - 1s 312us/step - loss: 7.8414e-04 - val_loss: 0.0014\n",
            "Epoch 158/200\n",
            "4236/4236 [==============================] - 1s 311us/step - loss: 7.8419e-04 - val_loss: 0.0013\n",
            "Epoch 159/200\n",
            "4236/4236 [==============================] - 1s 308us/step - loss: 7.7170e-04 - val_loss: 0.0013\n",
            "Epoch 160/200\n",
            "4236/4236 [==============================] - 1s 308us/step - loss: 8.1561e-04 - val_loss: 0.0013\n",
            "Epoch 161/200\n",
            "4236/4236 [==============================] - 1s 308us/step - loss: 8.1930e-04 - val_loss: 0.0014\n",
            "Epoch 162/200\n",
            "4236/4236 [==============================] - 1s 309us/step - loss: 8.4209e-04 - val_loss: 0.0012\n",
            "Epoch 163/200\n",
            "4236/4236 [==============================] - 1s 308us/step - loss: 8.7027e-04 - val_loss: 0.0013\n",
            "Epoch 164/200\n",
            "4236/4236 [==============================] - 1s 304us/step - loss: 7.8899e-04 - val_loss: 0.0014\n",
            "Epoch 165/200\n",
            "4236/4236 [==============================] - 1s 307us/step - loss: 8.0062e-04 - val_loss: 0.0013\n",
            "Epoch 166/200\n",
            "4236/4236 [==============================] - 1s 302us/step - loss: 7.5474e-04 - val_loss: 0.0013\n",
            "Epoch 167/200\n",
            "4236/4236 [==============================] - 1s 313us/step - loss: 7.4520e-04 - val_loss: 0.0012\n",
            "Epoch 168/200\n",
            "4236/4236 [==============================] - 1s 306us/step - loss: 7.9283e-04 - val_loss: 0.0013\n",
            "Epoch 169/200\n",
            "4236/4236 [==============================] - 1s 310us/step - loss: 7.8597e-04 - val_loss: 0.0013\n",
            "Epoch 170/200\n",
            "4236/4236 [==============================] - 1s 309us/step - loss: 7.7670e-04 - val_loss: 0.0013\n",
            "Epoch 171/200\n",
            "4236/4236 [==============================] - 1s 304us/step - loss: 7.4689e-04 - val_loss: 0.0012\n",
            "Epoch 172/200\n",
            "4236/4236 [==============================] - 1s 314us/step - loss: 7.8780e-04 - val_loss: 0.0012\n",
            "Epoch 173/200\n",
            "4236/4236 [==============================] - 1s 312us/step - loss: 8.2969e-04 - val_loss: 0.0021\n",
            "Epoch 174/200\n",
            "4236/4236 [==============================] - 1s 331us/step - loss: 8.3872e-04 - val_loss: 0.0012\n",
            "Epoch 175/200\n",
            "4236/4236 [==============================] - 1s 312us/step - loss: 8.4519e-04 - val_loss: 0.0012\n",
            "Epoch 176/200\n",
            "4236/4236 [==============================] - 1s 312us/step - loss: 7.3318e-04 - val_loss: 0.0012\n",
            "Epoch 177/200\n",
            "4236/4236 [==============================] - 1s 306us/step - loss: 7.7183e-04 - val_loss: 0.0013\n",
            "Epoch 178/200\n",
            "4236/4236 [==============================] - 1s 310us/step - loss: 7.1519e-04 - val_loss: 0.0013\n",
            "Epoch 179/200\n",
            "4236/4236 [==============================] - 1s 309us/step - loss: 8.0977e-04 - val_loss: 0.0012\n",
            "Epoch 180/200\n",
            "4236/4236 [==============================] - 1s 304us/step - loss: 7.3011e-04 - val_loss: 0.0012\n",
            "Epoch 181/200\n",
            "4236/4236 [==============================] - 1s 316us/step - loss: 7.1711e-04 - val_loss: 0.0014\n",
            "Epoch 182/200\n",
            "4236/4236 [==============================] - 1s 310us/step - loss: 7.8114e-04 - val_loss: 0.0015\n",
            "Epoch 183/200\n",
            "4236/4236 [==============================] - 1s 305us/step - loss: 7.3620e-04 - val_loss: 0.0013\n",
            "Epoch 184/200\n",
            "4236/4236 [==============================] - 1s 305us/step - loss: 7.2067e-04 - val_loss: 0.0012\n",
            "Epoch 185/200\n",
            "4236/4236 [==============================] - 1s 306us/step - loss: 8.1612e-04 - val_loss: 0.0014\n",
            "Epoch 186/200\n",
            "4236/4236 [==============================] - 1s 336us/step - loss: 7.7311e-04 - val_loss: 0.0012\n",
            "Epoch 187/200\n",
            "4236/4236 [==============================] - 1s 304us/step - loss: 6.9601e-04 - val_loss: 0.0013\n",
            "Epoch 188/200\n",
            "4236/4236 [==============================] - 1s 305us/step - loss: 8.1976e-04 - val_loss: 0.0012\n",
            "Epoch 189/200\n",
            "4236/4236 [==============================] - 1s 320us/step - loss: 7.9718e-04 - val_loss: 0.0017\n",
            "Epoch 190/200\n",
            "4236/4236 [==============================] - 1s 309us/step - loss: 7.8819e-04 - val_loss: 0.0013\n",
            "Epoch 191/200\n",
            "4236/4236 [==============================] - 1s 304us/step - loss: 7.7260e-04 - val_loss: 0.0012\n",
            "Epoch 192/200\n",
            "4236/4236 [==============================] - 1s 301us/step - loss: 7.6341e-04 - val_loss: 0.0014\n",
            "Epoch 193/200\n",
            "4236/4236 [==============================] - 1s 305us/step - loss: 7.6828e-04 - val_loss: 0.0012\n",
            "Epoch 194/200\n",
            "4236/4236 [==============================] - 1s 305us/step - loss: 6.7842e-04 - val_loss: 0.0012\n",
            "Epoch 195/200\n",
            "4236/4236 [==============================] - 1s 310us/step - loss: 7.0704e-04 - val_loss: 0.0012\n",
            "Epoch 196/200\n",
            "4236/4236 [==============================] - 1s 304us/step - loss: 6.9027e-04 - val_loss: 0.0013\n",
            "Epoch 197/200\n",
            "4236/4236 [==============================] - 1s 308us/step - loss: 8.0724e-04 - val_loss: 0.0015\n",
            "Epoch 198/200\n",
            "4236/4236 [==============================] - 1s 336us/step - loss: 8.2804e-04 - val_loss: 0.0014\n",
            "Epoch 199/200\n",
            "4236/4236 [==============================] - 1s 306us/step - loss: 7.3046e-04 - val_loss: 0.0013\n",
            "Epoch 200/200\n",
            "4236/4236 [==============================] - 1s 317us/step - loss: 7.3203e-04 - val_loss: 0.0012\n",
            "All done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV0V07-LVMTn",
        "colab_type": "text"
      },
      "source": [
        "## Testing\n",
        "* If the prediction is correct, the dots should form 45 degree diagonal line(y_pred = y_test).\n",
        "* If the prediction sign is correct, the dots should be in first or third quadrant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUWr-j2G08Eu",
        "colab_type": "code",
        "outputId": "3472b10a-1c84-4d55-9af3-048fb602c21a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "K.clear_session()\n",
        "net_generator = net_gen.next()\n",
        "modelconfig=next(net_generator,None)\n",
        "modelname,model = modelconfig\n",
        "model.load_weights('log/'+modelname+'_weights.h5')\n",
        "\n",
        "x_eval = x_test\n",
        "y_eval = y_test\n",
        "\n",
        "\n",
        "y_pred = model.predict(x_eval)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(y_pred.reshape(-1), y_eval.reshape(-1) )\n",
        "# plt.plot(range(y_pred.shape[0]),y_pred[:,0],color='blue')\n",
        "# plt.plot(range(y_eval.shape[0]),y_eval[:,0],color='deepskyblue')\n",
        "\n",
        "print('fraction of test cases with positive price change in future = {}'.format(len(np.where(y_eval>0)[0])/len(y_eval)))\n",
        "print('predicted price movement direction accuracy = {}'.format(len(np.where(np.multiply(y_eval,y_pred)>0)[0])/len(y_eval)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fraction of test cases with positive price change in future = 0.5666666666666667\n",
            "predicted price movement direction accuracy = 0.4766666666666667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmIAAAI/CAYAAADURrXPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDACgIYIYAEBDBDEAgIb8/xZto+t7QLpBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFdJ-Nq2JWQ1",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "* The model could not predict price change in future (see graph above).\n",
        "* This may be logical as someone conjured that stock price as \"a random walk\".\n",
        "* You may try a different history window or use a different model."
      ]
    }
  ]
}